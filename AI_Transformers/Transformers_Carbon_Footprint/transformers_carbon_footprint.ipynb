{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [View Solution Notebook](./solution.html)\n",
    "- [View Project Page]()\n",
    "\n",
    "# Imports and Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Instantiate DistilGPT-2's `tokenizer` and `model` using the `.from_pretrained` method.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31373,    11,   995,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 2. Assign pt_tensors the input text's tokens in PyTorch tensor form\n",
    "def encode_text_as_pt_tensor(text):\n",
    "    pt_tensors = tokenizer.encode(text, return_tensors = 'pt')\n",
    "    return pt_tensors\n",
    "\n",
    "print(encode_text_as_pt_tensor(\"hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# 3. Use set_seed to make the rest of the notebook's output deterministic. Pass it the number 42.\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Your prompt here!\"\n",
    "tokens = encode_text_as_pt_tensor(prompt)\n",
    "# 4. Instruct the model to generate a completion on your choice of prompt using the greedy search method.\n",
    "output_tokens = model.generate(tokens, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your prompt here!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Decode the resulting tokens.\n",
    "def decode_tokens(tokens):\n",
    "    text = tokenizer.decode(tokens)\n",
    "    return text\n",
    "\n",
    "decode_tokens(output_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your prompt here.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "your prompt here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "your prompt here\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'your prompt here\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Adapt the function below to use beam search in its generations. Then call it three times with 2 beam, 6 beams, and 14 beams.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "def generate_with_beam_search(prompt,num_beams):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, num_beams = num_beams,\n",
    "                            pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "\n",
    "prompt = \"your prompt here\"\n",
    "\n",
    "generate_with_beam_search(prompt, 2)\n",
    "generate_with_beam_search(prompt, 6)\n",
    "generate_with_beam_search(prompt, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your prompt here.\n",
      "\n",
      "If you have any questions, please let us know in the comments below\n",
      "your prompt here.\n",
      "\n",
      "\n",
      "If you have any questions, please let us know in the comments\n",
      "your prompt here.\n",
      "\n",
      "\n",
      "\n",
      "If you have any questions, please feel free to contact me\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'your prompt here.\\n\\n\\n\\nIf you have any questions, please feel free to contact me'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Repeat the same process you did with beam search on step 6 with n-gram penalties here.\n",
    "# Call our function three times with n_gram values of 2, 3, and 4.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "def generate_with_ngram_penalty(prompt, n_gram_penalty, num_beams=6):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, num_beams = num_beams,\n",
    "                            no_repeat_ngram_size = n_gram_penalty,\n",
    "                            pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "generate_with_ngram_penalty(prompt, 2)\n",
    "generate_with_ngram_penalty(prompt, 3)\n",
    "generate_with_ngram_penalty(prompt, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      " your prompt here:\n",
      "\n",
      "This is the final step to make sure your app works as expected.\n",
      "Temperature: 0.8\n",
      "Top K: 30\n",
      " your prompt here is for you to read the guide and to try to get back to your original state\n",
      "Temperature: 0.9\n",
      "Top K: 20\n",
      " your prompt here.\n",
      "\n",
      "\n",
      "\n",
      "You will be asked to fill out the form to help the recipient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'your prompt here.\\n\\n\\n\\nYou will be asked to fill out the form to help the recipient'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Same as steps 6 and 7, experiment with different settings of temperature and top_k here after instructing the model to do sampling.\n",
    "# Choose your own values for temperature and top k and see how the model's output responds.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "\n",
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty = 2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, no_repeat_ngram_size = n_gram_penalty,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample=True, temperature=temperature,\n",
    "                            top_k=top_k)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n {completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling(prompt, 0.6, 50)\n",
    "generate_with_sampling(prompt, 0.8, 30)\n",
    "generate_with_sampling(prompt, 0.9, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CodeCarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:50:04] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:50:04] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:50:04] No GPU found.\n",
      "[codecarbon INFO @ 21:50:04] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:50:04] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 21:50:05] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 21:50:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:50:05]   Platform system: Linux-4.14.352-268.568.amzn2.x86_64-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 21:50:05]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 21:50:05]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 21:50:05]   Available RAM : 62.118 GB\n",
      "[codecarbon INFO @ 21:50:05]   CPU count: 16\n",
      "[codecarbon INFO @ 21:50:05]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 21:50:05]   GPU count: None\n",
      "[codecarbon INFO @ 21:50:05]   GPU model: None\n",
      "[codecarbon INFO @ 21:50:09] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 21:50:09] Energy consumed for RAM : 0.000005 kWh. RAM Power : 23.294320106506348 W\n",
      "[codecarbon INFO @ 21:50:09] Energy consumed for all CPUs : 0.000025 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 21:50:09] 0.000030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:50:09] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      " Carbon dioxide is a greenhouse gas, which emits a significant amount of greenhouse gases.\n",
      "\n",
      "The\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Carbon dioxide is a greenhouse gas, which emits a significant amount of greenhouse gases.\\n\\nThe'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from codecarbon import track_emissions\n",
    "\n",
    "# 9. Add the CodeCarbon decorator to the line directly above this function.\n",
    "# Then fill the function in with your preferred settings.\n",
    "\n",
    "# add the decorator here\n",
    "@track_emissions\n",
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, no_repeat_ngram_size = n_gram_penalty,\n",
    "                            pad_token_id = tokenizer.eos_token_id,\n",
    "                            do_sample = True, temperature = temperature,\n",
    "                            top_k = top_k)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n {completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling(\"Carbon dioxide is a\", 0.6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp project_name                                run_id  \\\n",
      "0  2024-10-10T21:39:41   codecarbon  b8b6c1e0-b1f4-40ed-8eba-232b8b308a64   \n",
      "1  2024-10-10T21:50:09   codecarbon  ed261d56-e0cc-4581-9b0b-ae73429e7117   \n",
      "\n",
      "   duration  emissions  emissions_rate  cpu_power  gpu_power  ram_power  \\\n",
      "0  0.895004   0.000011        0.000012      105.0        0.0   23.29432   \n",
      "1  0.844067   0.000011        0.000013      105.0        0.0   23.29432   \n",
      "\n",
      "   cpu_energy  ...  cpu_count                                       cpu_model  \\\n",
      "0    0.000024  ...         16  Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz   \n",
      "1    0.000025  ...         16  Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz   \n",
      "\n",
      "   gpu_count gpu_model longitude latitude  ram_total_size  tracking_mode  \\\n",
      "0        NaN       NaN  -77.4903  39.0469       62.118187        machine   \n",
      "1        NaN       NaN  -77.4903  39.0469       62.118187        machine   \n",
      "\n",
      "  on_cloud  pue  \n",
      "0        N  1.0  \n",
      "1        N  1.0  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 10. Use pandas' read_csv method to load in the emissions.csv we generated.\n",
    "emissions = pd.read_csv('emissions.csv')\n",
    "print(emissions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
